{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the XPT file into a pandas DataFrame\n",
    "import pandas as pd\n",
    "df_2024 = pd.read_sas('LLCP2024.XPT')\n",
    "# df_2023 = pd.read_sas('LLCP2023.XPT')\n",
    "# df_2022 = pd.read_sas('LLCP2022.XPT')\n",
    "# Display the first 5 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_2024.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print list of columns and count for df_2024_selected\n",
    "cols = df_2024.columns.tolist()\n",
    "print(f\"{len(cols)} columns:\")\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where DISPCODE == 1100 and save in df\n",
    "if 'DISPCODE' not in df_2024.columns:\n",
    "    raise KeyError(\"DISPCODE column not found in df_2024\")\n",
    "\n",
    "df = df_2024[df_2024['DISPCODE'] == 1100].copy()\n",
    "print(\"df.shape =\", df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['_MICHD','_ASTHMS1','_DRDXAR2','DIABETE4','_PHYS14D','_TOTINDA','_SEX','_AGE_G','_BMI5CAT','_EDUCAG','_INCOMG1','_RFSMOK3','DRNKANY6','SSBSUGR2']\n",
    "df_2024_new = df.reindex(columns= selected_columns).copy()\n",
    "\n",
    "# report results\n",
    "missing_cols = [c for c in selected_columns if c not in df_2024.columns]\n",
    "print(\"df_2024_new.shape =\", df_2024_new.shape)\n",
    "if missing_cols:\n",
    "    print(\"Missing columns in df_2024:\", missing_cols)\n",
    "\n",
    "# quick preview\n",
    "display(df_2024_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading underscores from all column names in df_2024_new\n",
    "df_2024_new.rename(columns=lambda c: c.lstrip('_') if isinstance(c, str) else c, inplace=True)\n",
    "# quick check\n",
    "print(df_2024_new.columns.tolist())\n",
    "df_2024_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "missing_values_map = {\n",
    "    'MICHD': [7, 9],\n",
    "    'PHYS14D': [9],\n",
    "    'TOTINDA': [9],\n",
    "    'ASTHMS1': [9],\n",
    "    'DRDXAR2': [7, 9],\n",
    "    'EDUCAG': [9],\n",
    "    'INCOMG1': [9],\n",
    "    'RFSMOK3': [9],\n",
    "    'DRNKANY6': [7, 9],\n",
    "    'SSBSUGR2': [777, 999],\n",
    "    'DIABETE4': [9,7,2]\n",
    "}\n",
    "\n",
    "# --- Loop through the map and replace the codes with np.nan ---\n",
    "# np.nan is the standard representation for NULL in pandas.\n",
    "for column, codes in missing_values_map.items():\n",
    "    if column in df_2024_new.columns:\n",
    "        # The .replace() method can take a list of values to be replaced\n",
    "        df_2024_new[column] = df_2024_new[column].replace(codes, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_2024_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Define and Apply the Categorization Logic ---\n",
    "\n",
    "# Define the conditions (bins) for each category\n",
    "conditions = [\n",
    "    (df_cleaned['SSBSUGR2'] >= 101) & (df_cleaned['SSBSUGR2'] <= 199),\n",
    "    (df_cleaned['SSBSUGR2'] >= 201) & (df_cleaned['SSBSUGR2'] <= 299),\n",
    "    ((df_cleaned['SSBSUGR2'] >= 301) & (df_cleaned['SSBSUGR2'] <= 399)) | (df_cleaned['SSBSUGR2'] == 888)\n",
    "]\n",
    "\n",
    "# Define the category labels that correspond to the conditions\n",
    "categories = ['High', 'Medium', 'Low']\n",
    "\n",
    "# Use np.select to create the new column based on the conditions\n",
    "# The 'default' argument handles any case that doesn't meet a condition\n",
    "df_cleaned['SSBSUGR2_CAT'] = np.select(conditions, categories, default='Uncategorized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"SSBSUGR2_CAT\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a code to save df_cleaned to a CSV file named 'cleaned_data_2024.csv' without the index column.\n",
    "df_cleaned.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the cleaned CSV produced earlier\n",
    "df_loaded = pd.read_csv('cleaned_data.csv')\n",
    "# drop the specified columns and save to a new DataFrame\n",
    "df_dib = df_loaded.drop(columns=['SSBSUGR2', 'MICHD', 'ASTHMS1', 'DRDXAR2'])\n",
    "df_dib.shape\n",
    "display(df_dib.head())\n",
    "df_heart= df_loaded.drop(columns=['SSBSUGR2','MICHD','ASTHMS1','DIABETE4'])\n",
    "df_heart.shape\n",
    "display(df_heart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col=['PHYS14D','DIABETE4', 'TOTINDA', 'SEX', 'AGE_G', 'BMI5CAT', 'EDUCAG', 'INCOMG1', 'RFSMOK3', 'DRNKANY6']\n",
    "for numerical_col in df_dib.columns:\n",
    "    # attempt numeric conversion (coerce non-numeric -> NaN)\n",
    "    coerced = pd.to_numeric(df_dib[numerical_col], errors='coerce')\n",
    "    if not coerced.isna().all():\n",
    "        df_dib[numerical_col] = coerced.astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col_heart=['PHYS14D','DRDXAR2', 'TOTINDA', 'SEX', 'AGE_G', 'BMI5CAT', 'EDUCAG', 'INCOMG1', 'RFSMOK3', 'DRNKANY6']\n",
    "for numerical_col_heart in df_heart.columns:\n",
    "    # attempt numeric conversion (coerce non-numeric -> NaN)\n",
    "    coerced = pd.to_numeric(df_heart[numerical_col_heart], errors='coerce')\n",
    "    if not coerced.isna().all():\n",
    "        df_heart[numerical_col_heart] = coerced.astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dib.head()\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "ordinal_cols = ['SSBSUGR2_CAT']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
    "# fit_transform returns a numpy array; convert to a pandas Series and cast to nullable Int64\n",
    "df_dib[\"SSBSUGR2_en\"] = pd.Series(ordinal_encoder.fit_transform(df_dib[ordinal_cols]).ravel()).astype(\"Int64\")\n",
    "df_heart[\"SSBSUGR2_en\"] = pd.Series(ordinal_encoder.fit_transform(df_heart[ordinal_cols]).ravel()).astype(\"Int64\")\n",
    "df_dib.head()\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dib= df_dib.drop(columns=['SSBSUGR2_CAT'])\n",
    "df_heart= df_heart.drop(columns=['SSBSUGR2_CAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dib.head()\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dib['DIABETE4'].value_counts()\n",
    "df_heart['DRDXAR2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dib.to_csv('df_dib.csv', index=False)\n",
    "# df_heart.to_csv('df_heart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dib = pd.read_csv('df_dib.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_dib.drop('DIABETE4', axis=1)\n",
    "y = df_dib['DIABETE4']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "models = [\n",
    "    (\"LogisticRegression\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"RandomForest\", RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)),\n",
    "    (\"HistGradientBoosting\", HistGradientBoostingClassifier(random_state=42)),\n",
    "    (\"GradientBoosting\", GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    print(f\"{name} - Train Accuracy: {train_score:.4f}, Test Accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, model in models:\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # --- Get Scores for Training Data ---\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    \n",
    "    # --- Get Predictions for Test Data ---\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Get probability estimates for the 'positive' class (class 1)\n",
    "    # Note: predict_proba returns [prob_class_0, prob_class_1]\n",
    "    try:\n",
    "        y_proba_test = model.predict_proba(X_test_scaled)\n",
    "    except AttributeError:\n",
    "        # Some models (like KNN by default) might not have predict_proba\n",
    "        # Or handle it differently. For this example, we'll skip AUC if not available.\n",
    "        print(f\"\\nCould not get probabilities for {name}. Skipping ROC AUC.\")\n",
    "        y_proba_test = None\n",
    "\n",
    "    # --- Calculate Test Metrics ---\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    if y_proba_test is not None:\n",
    "        test_roc_auc = roc_auc_score(y_test, y_proba_test, multi_class='ovr', average='weighted')\n",
    "        \n",
    "        # Store data needed for plotting\n",
    "        # We store lists for JSON serialization\n",
    "        results[name] = {\n",
    "            'y_true': y_test.tolist(), \n",
    "            'y_proba': y_proba_test.tolist(), \n",
    "            'roc_auc': test_roc_auc\n",
    "        }\n",
    "    else:\n",
    "        test_roc_auc = \"N/A\"\n",
    "        results[name] = None # Mark as not plottable\n",
    "\n",
    "    # --- Print Results ---\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"  Train Accuracy: {train_score:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_score:.4f}\")\n",
    "    print(f\"  Test F1 Score:  {test_f1:.4f}\")\n",
    "    if y_proba_test is not None:\n",
    "        print(f\"  Test ROC AUC:   {test_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(max_leaf_nodes=70, max_iter=300, learning_rate=0.01)\n",
    "\n",
    "# Fit the model on your training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate scores\n",
    "train_score = model.score(X_train_scaled, y_train)\n",
    "test_score = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_score:.4f}, Test Accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate and print AUC-ROC score\n",
    "    # Using 'ovr' (One-vs-Rest) for multi-class and 'macro' average\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    # --- NEW: Generate Classification Report ---\n",
    "    # This report includes Precision, Recall, and F1-Score\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "print(f\"--- {name} ---\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Test AUC (OvR): {test_auc:.4f}\")\n",
    "print(\"\\n  --- Classification Report ---\")\n",
    "    # Indent the report for better readability\n",
    "report_lines = report.split('\\n')\n",
    "for line in report_lines:\n",
    "    print(f\"    {line}\")\n",
    "print(\"-\" * (len(name) + 8) + \"\\n\") # Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUC for the trained model on the test set (multi-class OvR + per-class AUC)\n",
    "proba = model.predict_proba(X_test_scaled)  # shape (n_samples, n_classes)\n",
    "\n",
    "# Macro (averaged) multi-class AUC (One-vs-Rest)\n",
    "auc_macro = roc_auc_score(y_test, proba, multi_class='ovr', average='macro')\n",
    "auc_weighted = roc_auc_score(y_test, proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "print(f\"Multi-class AUC (macro)   : {auc_macro:.4f}\")\n",
    "print(f\"Multi-class AUC (weighted): {auc_weighted:.4f}\")\n",
    "\n",
    "# Per-class (binary OvR) AUCs\n",
    "for i, cls in enumerate(model.classes_):\n",
    "    auc_cls = roc_auc_score((y_test == cls).astype(int), proba[:, i])\n",
    "    print(f\"AUC for class {cls} vs rest: {auc_cls:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    " \n",
    "#  Save scaler and model separately\n",
    "joblib.dump(scaler, \"scaler_hist_dib.pkl\")\n",
    "joblib.dump(model, \"hist_model_dib.pkl\")\n",
    "print(\"âœ… Scaler and model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "models = {\n",
    "    # \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    # \"RandomForest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Parameter distributions for RandomizedSearchCV\n",
    "# Note: Using distributions (like uniform, randint) is possible but for simplicity, we use lists of values.\n",
    "param_grids = {\n",
    "    # \"LogisticRegression\": {\n",
    "    #     'C': [0.01, 0.1, 1, 10, 100],\n",
    "    #     'solver': ['liblinear', 'saga']\n",
    "    # },\n",
    "    # \"RandomForest\": {\n",
    "    #     'n_estimators': [100, 200, 300],\n",
    "    #     'max_depth': [None, 10, 20, 30],\n",
    "    #     'min_samples_split': [2, 5, 10],\n",
    "    #     'min_samples_leaf': [1, 2, 4]\n",
    "    # },\n",
    "    \"HistGradientBoosting\": {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_iter': [100, 200, 300],\n",
    "        'max_leaf_nodes': [31, 50, 70]\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 3. Run Hyperparameter Tuning ---\n",
    "\n",
    "# Store results\n",
    "best_models = {}\n",
    "\n",
    "for name in models:\n",
    "    print(f\"--- Tuning {name} ---\")\n",
    "    \n",
    "    # Initialize RandomizedSearchCV\n",
    "    # n_iter controls how many different parameter combinations are tried.\n",
    "    # cv is the number of folds in cross-validation.\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=models[name],\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=10,  # Increase for more thorough search\n",
    "        cv=3,       # Use 3-fold cross-validation\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1   # Use all available CPU cores\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Store the best estimator\n",
    "    best_models[name] = random_search.best_estimator_\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best CV Score (Accuracy): {random_search.best_score_:.4f}\")\n",
    "    print(\"-\" * (len(name) + 12) + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Evaluate Best Models on Test Set ---\n",
    "print(\"\\n--- Evaluating Best Models on Test Data ---\\n\")\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  - Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
